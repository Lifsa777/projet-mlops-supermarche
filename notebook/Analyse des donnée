# src/train.py
# Objectif: prédire un surplus (gaspillage) à la fin de la journée à partir de variables connues le matin

import json
from pathlib import Path
import pandas as pd
import numpy as np

from sklearn.model_selection import TimeSeriesSplit
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# --------------------
# 1) Chargement
# --------------------
DATA = Path("data") / "sales_data.csv"
MODELS = Path("models"); MODELS.mkdir(exist_ok=True)

df = pd.read_csv(DATA)
df["date"] = pd.to_datetime(df["date"], errors="coerce")
df = df.dropna(subset=["date"])  # garde les lignes avec date valide

# --------------------
# 2) Feature engineering
# --------------------
df["jour_de_semaine"] = df["date"].dt.dayofweek
df["mois"] = df["date"].dt.month

# (Option) borne le surplus à >=0 s'il peut être négatif
# df["surplus"] = df["surplus"].clip(lower=0)

# --------------------
# 3) Features / cible
# --------------------
features = ["product_id", "prix", "stock_initial", "jours_avant_peremption",
            "jour_de_semaine", "mois"]
target = "surplus"

X = df[features].copy()
y = df[target].values

# Déclarer les colonnes catégorielles / numériques
cat_cols = ["product_id"]  # identifiant produit: catégoriel !
num_cols = [c for c in X.columns if c not in cat_cols]

# --------------------
# 4) Prétraitement + Modèle dans un Pipeline
# --------------------
preprocess = ColumnTransformer([
    ("num", SimpleImputer(strategy="median"), num_cols),
    ("cat", Pipeline([
        ("imp", SimpleImputer(strategy="most_frequent")),
        ("ohe", OneHotEncoder(handle_unknown="ignore"))
    ]), cat_cols)
])

model = RandomForestRegressor(
    n_estimators=300,
    max_depth=None,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)

pipe = Pipeline([
    ("prep", preprocess),
    ("rf", model)
])

# --------------------
# 5) Split temporel (dernier fold en test)
# --------------------
# Trie par date pour éviter toute fuite temporelle
df_sorted = df.sort_values("date").reset_index(drop=True)
X_sorted = X.loc[df_sorted.index]
y_sorted = y[df_sorted.index]

tscv = TimeSeriesSplit(n_splits=5)
tr_idx, te_idx = list(tscv.split(X_sorted))[-1]  # dernier split = test chronologique

X_train, X_test = X_sorted.iloc[tr_idx], X_sorted.iloc[te_idx]
y_train, y_test = y_sorted[tr_idx], y_sorted[te_idx]

# --------------------
# 6) Entraînement + Évaluation
# --------------------
print("Entraînement du modèle RandomForest...")
pipe.fit(X_train, y_train)
pred = pipe.predict(X_test)

mae  = mean_absolute_error(y_test, pred)
rmse = mean_squared_error(y_test, pred, squared=False)
r2   = r2_score(y_test, pred)

print(f"MAE: {mae:.2f} | RMSE: {rmse:.2f} | R2: {r2:.3f}")

# --------------------
# 7) Sauvegarde artefacts (modèle + métriques)
# --------------------
model_path = MODELS / "surplus_rf.joblib"
joblib.dump(pipe, model_path)
print(f"Modèle sauvegardé -> {model_path}")

metrics = {"MAE": mae, "RMSE": rmse, "R2": r2, "n_test": int(len(y_test))}
with open(MODELS / "metrics_surplus_rf.json", "w", encoding="utf-8") as f:
    json.dump(metrics, f, ensure_ascii=False, indent=2)
print("Métriques ->", MODELS / "metrics_surplus_rf.json")
